{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsoldadomagraner/TReND-CaMinA/blob/main/notebooks/Rwanda24/18-Thu-ProbModels/probabilistic_models_LDS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Probabilistic models: Linear Dynamical Systems**\n",
        "\n",
        "<img align=\"left\" width=\"100\" src=\"https://raw.githubusercontent.com/trendinafrica/TReND-CaMinA/main/images/CaMinA_logo.png\">\n",
        "\n",
        "### **TReND-CaMinA: Computational Neuroscience and Machine Learning basics school, Rwanda 2024**\n",
        "#### **Content creator:** Lea Duncker, Stanford University\n",
        "#### **Adapted by:** Joana Soldado Magraner, Carnegie Mellon University\n",
        "\n",
        "---\n",
        "\n",
        "###**Learning objectives:**\n",
        "* Generate time series data using an LDS, a simple probabilistic linear model\n",
        "* Gain intuitions about how complex high-dimensional neural data, and in particular neural trajectories, can be described with simple low-dimensional dynamical models\n",
        "* Apply algorithms to fit LDS models to time series data recorded from the brain"
      ],
      "metadata": {
        "id": "T8GXZiOimnuB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aog9YerU5eBA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 1:** Exploring properties of linear dynamics\n",
        "\n",
        "First, we will try to get some intuition about how linear dynamics and their properties shape the evolution of states over time. To do this, we will construct two-dimensional linear dynamics with different parameter settings, look at their eigenvalues and eigenvectors and simulate how the system evolves forward in time.\n",
        "\n",
        "A linear dynamical system evolves in time according to the equation\n",
        "$$ \\boldsymbol{x}_{t+1} = A \\boldsymbol{x}_{t} + \\boldsymbol{\\epsilon}_t$$\n",
        "Here, $\\boldsymbol{x} \\in \\mathbb{R}^{D}$ is the dynamical state variable, $A \\in \\mathbb{R}^{D\\times D}$ are the linear dynamics and $\\boldsymbol{\\epsilon} \\sim N(0,Q)$ is Gaussian noise, with $Q \\in \\mathbb{R}^{D\\times D}$ the state noise covariance. We will first explore this equation and how the evolution of $\\boldsymbol{x}$ depends on $A$ and $\\boldsymbol{\\epsilon}$ in more details.\n"
      ],
      "metadata": {
        "id": "FhgkUiJ455sr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Implement the deterministic part of the equation above, which takes a state $\\boldsymbol{x}$ and dynamics matrix $A$, and generates the next state as $A \\boldsymbol{x}$\n",
        "2. Implement a function in Python that takes an initial condition $\\boldsymbol{x}_0$ and the noise covariance $Q$ as input, and simulates $\\boldsymbol{x}$ forward as a noisy dynamical system for T time-steps, according to the above equation.\n"
      ],
      "metadata": {
        "id": "LS86Y9l_56Vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_trial(A, x0, T, Q):\n",
        "    # x0 is the [D,] initial condition\n",
        "    # T is the maximum number of timesteps we want to simulate\n",
        "    # Q is the [D, D] noise covariance\n",
        "\n",
        "    D = x0.shape[0]\n",
        "    x = np.zeros([D, T])\n",
        "\n",
        "    x[:, 0] = x0[:]\n",
        "    for t in range(T-1):\n",
        "        # x_t+1 = A * x_t + random multivariate normal noise with covariance Q\n",
        "        # ############ Edit this #############################\n",
        "        x[:, t + 1] =\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "921KlfH85qTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Double click to see solution {display-mode: \"form\" }\n",
        "\n",
        "def simulate_trial(A, x0, T, Q):\n",
        "    # x0 is the [D,] initial condition\n",
        "    # T is the maximum number of timesteps we want to simulate\n",
        "    # Q is the [D, D] noise covariance\n",
        "\n",
        "    D = x0.shape[0]\n",
        "    x = np.zeros([D, T])\n",
        "\n",
        "    x[:, 0] = x0[:]\n",
        "    for t in range(T-1):\n",
        "        # x_t+1 = A * x_t + random multivariate normal noise with covariance Q\n",
        "        # ############ Edit this #############################\n",
        "        x[:, t + 1] = A.dot(x[:, t]) + np.random.multivariate_normal(np.zeros([D]), Q, 1)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "6P33-w9JrIkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.   Explore how different choices for $A$ affect the state evolution. Below there are two functions that implement a rotation matrix or a random symmetric matrix. Some things to explore:\n",
        "*   Pick values for $\\boldsymbol{x}_0$, $Q$ and $A$ and simulate the dynamical system using the functions you wrote before\n",
        "*   What happens when $A$ is symmetric, or a rotation matrix? What looks different about the state evolution? How does that relate to the eigenvalues and eigenvectors of $A$?\n",
        "*   What happens when eigenvalues of $A$ are real or complex?\n",
        "*   How does the size of the largest eigenvalue affect the evolution?\n",
        "*   What changes when $Q$ is large or small?\n",
        "\n",
        "You should try to visualize the state evolution of $\\boldsymbol{x}$ as a time series, and through a plot in 2D. Look at the functon `plot_flowfield`, which uses `streamplot` as well to see if you can visualize the flow-field with the paths of $\\boldsymbol{x}_t$ overlaid on top.\n",
        "\n"
      ],
      "metadata": {
        "id": "L4ss6Srh-gfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# implement a 2D rotation around an angle theta\n",
        "def rotation(r, theta):\n",
        "    return r*np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
        "\n",
        "def symmetric(r):\n",
        "    a = np.random.randn(2,2)\n",
        "    u = np.linalg.svd(a)[0]\n",
        "    e = r* np.array([1., 0.5]) # scale the eigenvalues by r\n",
        "    return (u*e).dot(u.T)      # A real symmetric matrix fulfills U*E*U.T"
      ],
      "metadata": {
        "id": "lyjJaH5F50IT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make a linear system with a 2D rotation\n",
        "x0 = np.array([-1, 1]) # initial condition\n",
        "A_rot = rotation(0.99, 0.5) # dynamics matrix\n",
        "A_sym = symmetric(0.99) # dynamics matrix\n",
        "Q = 0.00 * np.eye(2) # innovations noise\n",
        "T = 100\n",
        "\n",
        "xpath_Rotation  = simulate_trial(A_rot, x0, T, Q)\n",
        "xpath_Symmetric = simulate_trial(A_sym, x0, T, Q)"
      ],
      "metadata": {
        "id": "shuT1tCb-jHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize simulated traces\n",
        "\n",
        "# plot state evolution under the rotation dynamics\n",
        "plt.plot(xpath_Rotation.T)\n",
        "plt.xlabel('time')\n",
        "plt.title('state evolution with 2D rotation dynamics')\n",
        "plt.show()\n",
        "\n",
        "# plot state evolution under the symmetric dynamics\n",
        "plt.plot(xpath_Symmetric.T)\n",
        "plt.xlabel('time')\n",
        "plt.title('state evolution with 2D symmetric dynamics')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dPOlssBn_BRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_flowfield(A,ngrid=20,w=2.5): #100, 2.5\n",
        "    xv, yv = np.meshgrid(np.linspace(-w,w,ngrid), np.linspace(-w,w,ngrid))\n",
        "    xygrid = np.hstack((xv.reshape(-1, 1), yv.reshape(-1, 1)))\n",
        "    xy_out = A.dot(xygrid.T).T - xygrid # dt = x_t+1 - x_t\n",
        "    U = xy_out[:,0].reshape(ngrid,ngrid)\n",
        "    V = xy_out[:,1].reshape(ngrid,ngrid)\n",
        "    speed = np.sqrt(U**2 + V**2)\n",
        "\n",
        "    #drawing stream plot\n",
        "    #plt.streamplot(xv, yv, U, V)\n",
        "    plt.quiver(xv, yv, U, V)\n",
        "    plt.axis('square')"
      ],
      "metadata": {
        "id": "ML-BSCJXbb9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot flow field with rotational dynamics and overlaid state evolution in 2D\n",
        "plt.title('rotational dynamics')\n",
        "\n",
        "plot_flowfield(A_rot, ngrid=20,w=2.5)\n",
        "# add marker for first point\n",
        "plt.plot(xpath_Rotation[0,0],xpath_Rotation[1,0],'g.',markersize=20)\n",
        "# add marker for last point\n",
        "plt.plot(xpath_Rotation[0,-1],xpath_Rotation[1,-1],'r.',markersize=20)\n",
        "\n",
        "# plot path\n",
        "plt.plot(xpath_Rotation[0,:],xpath_Rotation[1,:],'k')\n",
        "\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "plt.show()\n",
        "\n",
        "# plot flow field with symmetric dynamics and overlaid state evolution in 2D\n",
        "plt.title('symmetric dynamics')\n",
        "\n",
        "plot_flowfield(A_sym, ngrid=20,w=2.5)\n",
        "\n",
        "# add marker for first point\n",
        "plt.plot(xpath_Symmetric[0,0],xpath_Symmetric[1,0],'g.',markersize=20)\n",
        "# add marker for last point\n",
        "plt.plot(xpath_Symmetric[0,-1],xpath_Symmetric[1,-1],'r.',markersize=20)\n",
        "# plot path\n",
        "plt.plot(xpath_Symmetric[0,:],xpath_Symmetric[1,:],'k')\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H6pdJyOpbmO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 2:** Simulating a low dimensional linear dynamical system\n",
        "\n",
        "Now that we have a better understanding of how linear dynamical systems work, we can connect this to a model for high-dimensional neural data. To do this, we will use the same set up as we did for models like Factor Analysis. Here, the low-dimensional dynamical system is embedded within a high-dimensional space as follows\n",
        "\n",
        "$$ x_{t+1} = A x_{t} + \\epsilon_t, \\quad x \\in \\mathbb{R}^D$$\n",
        "\n",
        "$$ y_{t} = C x_{t} + \\eta_t, \\quad y \\in \\mathbb{R}^N$$\n",
        "\n",
        "$C$ is a $N \\times D$ matrix, where $D \\le N$, and $\\eta \\sim N(0,R)$ is noise. We'll assume that $R$, which is an $N \\times N$ matrix, is diagonal.\n",
        "\n",
        "1.   Generate a time series for $x$ and $y$ and visualize your results. To do this you should write a function that simulates from the LDS model.\n",
        "2.   Perform PCA on this data. Look at the dimensionality by plotting the singular values of the data covariance matrix. Visualise what the leading PC projections of the data look like."
      ],
      "metadata": {
        "id": "W-ssVZUab7XU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make a linear system with a 2D rotation\n",
        "x0 = np.array([-1, 1]) # initial condition\n",
        "A = rotation(0.99, 0.5) # dynamics matrix\n",
        "D = 2\n",
        "N = 10\n",
        "C = np.random.randn(N,D)\n",
        "Q = 0.1 * np.eye(D) # innovations noise\n",
        "R = 0.5*np.eye(N)\n",
        "T = 100\n",
        "\n",
        "\n",
        "def simulate_LDS(A, x0, T, Q, C, R):\n",
        "    xpath = simulate_trial(A, x0, T, Q)\n",
        "    # y_t = C * x_t + random multivariate normal noise with covariance R\n",
        "    # ######### Edit this #####################\n",
        "    ypath =\n",
        "    ##########################################\n",
        "    return ypath, xpath\n",
        "\n"
      ],
      "metadata": {
        "id": "jPGW7TR4byEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Double click to see solution {display-mode: \"form\" }\n",
        "\n",
        "# make a linear system with a 2D rotation\n",
        "x0 = np.array([-1, 1]) # initial condition\n",
        "A = rotation(0.99, 0.5) # dynamics matrix\n",
        "D = 2\n",
        "N = 10\n",
        "C = np.random.randn(N,D)\n",
        "Q = 0.1 * np.eye(D) # innovations noise\n",
        "R = 0.5*np.eye(N)\n",
        "T = 100\n",
        "\n",
        "\n",
        "def simulate_LDS(A, x0, T, Q, C, R):\n",
        "    xpath = simulate_trial(A, x0, T, Q)\n",
        "    # y_t = C * x_t + random multivariate normal noise with covariance R\n",
        "    # ######### Edit this #####################\n",
        "    ypath = C.dot(xpath) + np.random.multivariate_normal(np.zeros(N), R, T).T\n",
        "    ##########################################\n",
        "    return ypath, xpath\n",
        "\n"
      ],
      "metadata": {
        "id": "U1kUWOjlFUQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ypath, xpath = simulate_LDS(A, x0, T, Q, C, R) # simultate from the LDS model"
      ],
      "metadata": {
        "id": "IpYx8InZb4eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot ypath high-d output\n",
        "plt.plot(ypath.T)\n",
        "plt.title('high-d output time series')\n",
        "plt.xlabel('time')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# plot xpath low-d latents\n",
        "plt.plot(xpath.T)\n",
        "plt.title('low-d latent time series')\n",
        "plt.xlabel('time')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Y0T8qLY1cPtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We can compute eigenvalues using the SVD of the data covariance, YY.T (since Y is centered), and we can ignore the scaling (n-1) since we will compute eigenvalue ratios\n",
        "PCs = np.linalg.svd(ypath.dot(ypath.T))[0]\n",
        "svds = np.linalg.svd(ypath.dot(ypath.T))[1] #singular values = eigenvalues in this case\n",
        "\n",
        "# look at dimensionality\n",
        "# plot the cumulative fraction of variance captured by each PC\n",
        "# (hint: the fraction of variance in each PC is proportional to the singular value associated with that dimension)\n",
        "plt.plot(svds/sum(svds),'o-')\n",
        "plt.xlabel('dimension')\n",
        "plt.ylabel('fraction of variance explained')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "nPCs=4 # number of PCs to plot\n",
        "for i in range(nPCs):\n",
        "    plt.plot(PCs[:,i].dot(ypath))\n",
        "plt.title('data PC projections')\n",
        "plt.xlabel('time')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jpBPJQfGcXOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 3:** Maximum likelihood learning via the EM algorithm\n",
        "\n",
        "Now that we have noisy, high-dimensional data, we want to find a way to estimate the underlying dynamics and latent states from this data. In our example, we made the system up and we know the ground-truth, but in real world applications we need to trust that our algorithm will estimate the right parameters, given the assumptions we made about where the data came from. In this exercise, we will work to build an algorithm that allows us to input observed measurements ($y$) and extract estimates for $A$, $x$, etc. This algorithm is called the Expectation Maximization (EM) algorithm. We've already seen it for probabilistic PCA and Factor Analysis and now we will extend it to our LDS model. EM is the foundation for a bunch of different Machine Learning algorithms.\n",
        "\n",
        "Try to step through the functions below to try to understand what they are doing. `run_ssm_kalman` is implementing the Kalman Smoothing algorithm we discussed in the lecture.\n",
        "1. What part of the EM algorithm does this correspond to?\n",
        "2. What is the goal of running this function?\n",
        "\n",
        "The details here are very technical, so don't worry too much about what the function does on the inside. If you are interested though, there is extra reading and some more detailed notes on EM in the reading material in the Google drive folder [this is advanced stuff!]."
      ],
      "metadata": {
        "id": "gz4gObXqdCNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_ssm_kalman(X, y_init, Q_init, A, Q, C, R, mode='smooth'):\n",
        "    \"\"\"\n",
        "    Calculates kalman-smoother estimates of SSM state posterior.\n",
        "    :param X:       data, [d, t_max] numpy array\n",
        "    :param y_init:  initial latent state, [k,] numpy array\n",
        "    :param Q_init:  initial variance, [k, k] numpy array\n",
        "    :param A:       latent dynamics matrix, [k, k] numpy array\n",
        "    :param Q:       innovariations covariance matrix, [k, k] numpy array\n",
        "    :param C:       output loading matrix, [d, k] numpy array\n",
        "    :param R:       output noise matrix, [d, d] numpy array\n",
        "    :param mode:    'forw' or 'filt' for forward filtering, 'smooth' for also backward filtering\n",
        "    :return:\n",
        "    y_hat:      posterior mean estimates, [k, t_max] numpy array\n",
        "    V_hat:      posterior variances on y_t, [t_max, k, k] numpy array\n",
        "    V_joint:    posterior covariances between y_{t+1}, y_t, [t_max, k, k] numpy array\n",
        "    likelihood: conditional log-likelihoods log(p(x_t|x_{1:t-1})), [t_max,] numpy array\n",
        "    \"\"\"\n",
        "    d, k = C.shape\n",
        "    t_max = X.shape[1]\n",
        "\n",
        "    # dimension checks\n",
        "    assert np.all(X.shape == (d, t_max)), \"Shape of X must be (%d, %d), %s provided\" % (d, t_max, X.shape)\n",
        "    assert np.all(y_init.shape == (k,)), \"Shape of y_init must be (%d,), %s provided\" % (k, y_init.shape)\n",
        "    assert np.all(Q_init.shape == (k, k)), \"Shape of Q_init must be (%d, %d), %s provided\" % (k, k, Q_init.shape)\n",
        "    assert np.all(A.shape == (k, k)), \"Shape of A must be (%d, %d), %s provided\" % (k, k, A.shape)\n",
        "    assert np.all(Q.shape == (k, k)), \"Shape of Q must be (%d, %d), %s provided\" % (k, k, Q.shape)\n",
        "    assert np.all(C.shape == (d, k)), \"Shape of C must be (%d, %d), %s provided\" % (d, k, C.shape)\n",
        "    assert np.all(R.shape == (d, d)), \"Shape of R must be (%d, %d), %s provided\" % (d, k, R.shape)\n",
        "\n",
        "    y_filt = np.zeros((k, t_max))  # filtering estimate: \\hat(y)_t^t\n",
        "    V_filt = np.zeros((t_max, k, k))  # filtering variance: \\hat(V)_t^t\n",
        "    y_hat = np.zeros((k, t_max))  # smoothing estimate: \\hat(y)_t^T\n",
        "    V_hat = np.zeros((t_max, k, k))  # smoothing variance: \\hat(V)_t^T\n",
        "    K = np.zeros((t_max, k, X.shape[0]))  # Kalman gain\n",
        "    J = np.zeros((t_max, k, k))  # smoothing gain\n",
        "    likelihood = np.zeros(t_max)  # conditional log-likelihood: p(x_t|x_{1:t-1})\n",
        "\n",
        "    I_k = np.eye(k)\n",
        "\n",
        "    # forward pass\n",
        "\n",
        "    V_pred = Q_init\n",
        "    y_pred = y_init\n",
        "\n",
        "    for t in range(t_max):\n",
        "        x_pred_err = X[:, t] - C.dot(y_pred)\n",
        "        V_x_pred = C @V_pred@C.T + R\n",
        "        V_x_pred_inv = np.linalg.inv(V_x_pred)\n",
        "        likelihood[t] = -0.5 * (np.linalg.slogdet(2 * np.pi * (V_x_pred))[1] +\n",
        "                                x_pred_err.T.dot(np.linalg.solve(V_x_pred,x_pred_err)))\n",
        "\n",
        "        K[t] = np.linalg.solve(V_x_pred.T, C@V_pred.T).T\n",
        "\n",
        "\n",
        "        y_filt[:, t] = y_pred + K[t].dot(x_pred_err)\n",
        "        V_filt[t] = V_pred - K[t]@C@V_pred\n",
        "\n",
        "        # symmetrise the variance to avoid numerical drift\n",
        "        V_filt[t] = (V_filt[t] + V_filt[t].T) / 2.0\n",
        "\n",
        "        y_pred = A.dot(y_filt[:, t])\n",
        "        V_pred = A @V_filt[t]@A.T + Q\n",
        "\n",
        "    # backward pass\n",
        "\n",
        "    if mode == 'filt' or mode == 'forw':\n",
        "        # skip if filtering/forward pass only\n",
        "        y_hat = y_filt\n",
        "        V_hat = V_filt\n",
        "        V_joint = None\n",
        "    else:\n",
        "        V_joint = np.zeros_like(V_filt)\n",
        "        y_hat[:, -1] = y_filt[:, -1]\n",
        "        V_hat[-1] = V_filt[-1]\n",
        "\n",
        "        for t in range(t_max - 2, -1, -1):\n",
        "            J[t] = np.linalg.solve(A@V_filt[t]@A.T + Q, A@V_filt[t].T).T\n",
        "\n",
        "            y_hat[:, t] = y_filt[:, t] + J[t].dot((y_hat[:, t + 1] - A.dot(y_filt[:, t])))\n",
        "            V_hat[t] = V_filt[t] + J[t]@(V_hat[t + 1] - A@V_filt[t]@A.T - Q)@J[t].T\n",
        "\n",
        "        V_joint[-2] = (I_k - K[-1]@C)@A@V_filt[-2]\n",
        "\n",
        "        for t in range(t_max - 3, -1, -1):\n",
        "            V_joint[t] = V_filt[t + 1]@J[t].T + J[t + 1]@(V_joint[t + 1] - A@V_filt[t + 1])@J[t].T\n",
        "\n",
        "    return y_hat, V_hat, V_joint, likelihood"
      ],
      "metadata": {
        "id": "cqqA0Fh2fpm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `run_EM` implements the Expectation Maximization algorithm.  \n",
        "\n",
        "Step through each line of this code and make sure you conceptually understand what it is supposed to do.\n",
        "\n",
        "Can you map the different lines to the outline of the EM algorithm we talked about in the lecture?\n",
        "\n",
        "\n",
        "If you are interested in understanding why the parameter updates look the way they do, there are more notes in the Google drive folder [this is advanced stuff!]."
      ],
      "metadata": {
        "id": "e3aDnh7hfzqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_EM(X, y0, Q0, A, Q, C, R, maxiter=100, tol=1e-4):\n",
        "    # X is the d x T data matrix\n",
        "    # y0 is the initial condition for the latent state\n",
        "    # Q0 is the initial Covariance of the first time step\n",
        "    # A,Q,C,R are starting values for our parameters\n",
        "    # maxiter is the maximum number of iterations to perform\n",
        "    # tol is the convergence tolerance\n",
        "\n",
        "    D, K = C.shape\n",
        "    T = X.shape[1]\n",
        "    loglike = []\n",
        "\n",
        "    for i in tqdm(range(maxiter)):\n",
        "\n",
        "        Y, V, Vj, likelihood = run_ssm_kalman(X, y0, Q0, A, Q, C, R, mode='smooth')\n",
        "        loglike.append(sum(likelihood))\n",
        "\n",
        "        # make sure that likelihood increases and check convergence\n",
        "        if i > 0:\n",
        "            diff = loglike[i] - loglike[i-1]\n",
        "            if diff < 0:\n",
        "                warnings.warn(\"decrease in log-likelihood detected\")\n",
        "        else:\n",
        "            diff = []\n",
        "\n",
        "        if i > 0 and abs(diff) < tol:\n",
        "            print('EM has converged')\n",
        "            break\n",
        "\n",
        "        # perform parameter updates\n",
        "        yy = sum(V) + Y@Y.T\n",
        "        xy = X@Y.T\n",
        "        rr1 = X@X.T\n",
        "        aa1 = sum(V) + Y[:,:-1]@Y[:,:-1].T\n",
        "        aa2 = sum(Vj) + Y[:,1:]@Y[:,:-1].T\n",
        "        qq1 = sum(V[1:]) + Y[:,1:]@Y[:,1:].T\n",
        "\n",
        "        # C update\n",
        "        C = np.linalg.solve(yy.T, xy.T).T\n",
        "\n",
        "        # A update\n",
        "        A = np.linalg.solve(aa1.T,aa2.T).T\n",
        "\n",
        "        # R update\n",
        "        R = np.diag(np.diag((rr1 - C@xy.T)/T)) # diagonal matrix is more stable for real data application\n",
        "\n",
        "        # Q update\n",
        "        Q = (qq1 - aa2@A.T)/(T-1)\n",
        "\n",
        "        # symmetrize covariances for numerical stability\n",
        "        Q = 0.5*(Q + Q.T)\n",
        "        R = 0.5*(R + R.T)\n",
        "\n",
        "        # update initial conditions\n",
        "        y0 = Y[:,0]\n",
        "        Q0 = V[0] - y0.dot(y0.T)\n",
        "\n",
        "    return A, Q, C, R, y0, Q0, Y, V, Vj, loglike"
      ],
      "metadata": {
        "id": "TadtO813dA05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 4:** Applying EM to simulated data\n",
        "\n",
        "Now that we have our EM algorithm ready to go, we can apply it to our simulated example and try to get our initial parameters back.\n",
        "\n",
        "Start with an initial guess for the model parameters and run EM starting at these values.\n"
      ],
      "metadata": {
        "id": "u3rEGL8-gnun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize parameter settings\n",
        "\n",
        "# dimensionality of our simulation from ealier\n",
        "# D = 2\n",
        "# N = 10\n",
        "\n",
        "x0 = np.zeros(2)\n",
        "Q0 = np.eye(2)\n",
        "Ainit = 0.5*np.eye(2)\n",
        "# Cinit = np.random.randn(N,D) # random initialization\n",
        "Cinit = C[:] # initialization at the true parameter value\n",
        "Rinit = np.eye(N)\n",
        "Qinit = np.eye(D)\n",
        "\n",
        "# run EM\n",
        "Ahat, Qhat, Chat, Rhat, x0hat, Q0hat, X, V, Vj, loglike = run_EM(ypath, x0, Q0, Ainit, Qinit, Cinit, Rinit, maxiter=1000)"
      ],
      "metadata": {
        "id": "aHKjxwcHgzu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A good check in implementing EM algorithms is to make sure our log-likelihood increases with every iteration:"
      ],
      "metadata": {
        "id": "7S3DLdkKg8Mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the loglikelihood for EM. Does it increase at each iteration?\n",
        "plt.plot(loglike)\n",
        "plt.title('log-likelihood')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9gvGEv3Ag4qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 5:** Diagnostics and assesing goodness of fit\n",
        "\n",
        "Now that we've ran our algorithm, how can we tell if we learned the right thing? Since we made up the data until now, we know what our true system looks like. We can first check that the output of our algorithm aligns with the ground truth.\n",
        "\n",
        "\n",
        "1.   Plot the inferred latent variables along with the true underlying 2D system and see if they match.\n",
        "\n",
        "2.   Does our estimate $\\hat{A}$ have the same entries as our true generative $A$? Why would you expect this, or why not? Do the eigenvalues and eigenvectors match? Think about why looking at eigenvalues and eigenvectors is a better measure for assessing whether the parameters match\n",
        "\n",
        "3. Run EM again but with different initializations for $C$ (true vs. random, see comments in code). What changes?\n"
      ],
      "metadata": {
        "id": "FBKFkJVahFdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(2,1,1)\n",
        "plt.plot(X.T)\n",
        "plt.plot(xpath.T,'--')\n",
        "plt.title('inferred posterior means $\\pm$ 2 std. dev. and true latent paths')\n",
        "plt.xlabel('times')\n",
        "\n",
        "for i in range(2):\n",
        "    plt.fill_between(range(T), X[i,:] + 2*np.sqrt(V[:,i,i]),\n",
        "                X[i,:]-2*np.sqrt(np.sqrt(V[:,i,i])), alpha=0.2)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(X.T, xpath.T)\n",
        "plt.title('inferred posterior means vs true latent paths')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RMMvxUyvhJqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(1,2,1)\n",
        "plot_flowfield(A)\n",
        "plt.title('true flow field')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "# plot the estimated flow-field\n",
        "plot_flowfield(Ahat)\n",
        "plt.title('estimated flow field')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ynGbkWpNhZgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('true dynamics:')\n",
        "print(A)\n",
        "\n",
        "print('estimated dynamics:')\n",
        "print(Ahat)"
      ],
      "metadata": {
        "id": "25zZ0nPChhOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('true dynamics eigen decomp:')\n",
        "print(np.linalg.eig(A))\n",
        "\n",
        "print('estimated dynamics eigen decom:')\n",
        "print(np.linalg.eig(Ahat))"
      ],
      "metadata": {
        "id": "u40Cz-xOhkVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In real-world applications, we don't have access to the true underlying parameters. We want to look at data to estimate them and learn something new! So how can we assess whether our model is a good model? This is a tough question and still an active topic of debate and research.\n",
        "\n",
        "\n",
        "3.   Think about what makes a model a \"good\" model. Can you come up with a list of features you'd like a good model to have?\n",
        "\n",
        "A common measure is to assess a model by asking: how well does it fit the data? Another way to ask this question is: How much variability in our observed data can be explained by our model? This is sometimes also called \"the fraction of variance explained\".\n",
        "\n",
        "4.   Below we have code to compute the fraction of variance that is explained by our model in the simulated data. Evaluate this for our current model fit and make a note of the value, then repeat the data generation and fitting procedure while increasing the level of noise in the latent states and in the data. What do you see now?\n",
        "\n",
        "5.   Can you think of a way to report variance explained that would be less sensitive to noise variance?\n",
        "\n",
        "6. What happens to the fraction of variance explained when you increase the number of latent states in the model? (but you don't change the latent state dimensionality of the simulation) Why is that?\n"
      ],
      "metadata": {
        "id": "v0HDV89Mhp3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def varianceExplained(Y, Yhat):\n",
        "    Yc = Y - np.mean(Y,axis=1)[:,None]\n",
        "    diff = Y - Yhat\n",
        "    RSS = np.trace(diff.dot(diff.T))\n",
        "    TSS = np.trace(Yc.dot(Yc.T))\n",
        "    Rsq = 1 - RSS/TSS\n",
        "    return Rsq"
      ],
      "metadata": {
        "id": "qADeW5XIhqY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the variance explained between the inferred poserior mean <y>_q = C <x>_q and the data\n",
        "# <.>_q denotes the expected value under the posterior distribution\n",
        "ypost = Chat.dot(X) # inferred poserior mean <y>_q = C <x>_q\n",
        "varianceExplained(ypath, ypost)"
      ],
      "metadata": {
        "id": "kmg23B1vhusF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How does this number change when we use a function to predict forward in time using the learned model rather than the inferred latent variables? How does that depend on the noise we used in simulating the system?"
      ],
      "metadata": {
        "id": "paJVdaEFiMHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_path(A, x0, T):\n",
        "    # x_t+1 = f\n",
        "    # x0 is the [k,] initial condition\n",
        "    # T is the maximum number of timesteps we want to simulate\n",
        "\n",
        "    K = x0.shape[0]\n",
        "    x = np.zeros([K, T])\n",
        "    x[:, 0] = x0[:]\n",
        "    for i in range(T-1):\n",
        "        x[:, i + 1] = A.dot(x[:, i])\n",
        "\n",
        "    return x\n",
        "\n",
        "xpred = predict_path(Ahat, x0hat, T)\n",
        "\n",
        "plt.plot(X.T)\n",
        "for i in range(2):\n",
        "    plt.fill_between(range(T), X[i,:] + 2*np.sqrt(V[:,i,i]),\n",
        "                X[i,:]-2*np.sqrt(np.sqrt(V[:,i,i])), alpha=0.2)\n",
        "\n",
        "plt.plot(xpred.T,'--')\n",
        "plt.title('inferred posterior means and predictions forward from initial condition')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vQAEIj2WiLas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yexpl_pred = Chat.dot(xpred)\n",
        "varianceExplained(ypath,yexpl_pred)"
      ],
      "metadata": {
        "id": "JfcxSyvtiwvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 6:** Application to neural data\n",
        "\n",
        "We now have a full algorithm to take high-dimensional data and infer a low-dimensional time series of latent states, and the dynamical system that explains their evolution over time from data. So far we have only worked with toy data, but we are now ready to put our algorithm to the test and see if it can extract interesting structure from neural data.\n",
        "\n",
        "We will fit a dynamical system to trial-averaged data from motor cortex during movement execution. We've described the task in more detail in the lecture, so let's dig into the data now.\n",
        "\n",
        "You will need to download the dataset from the google drive folder and upload it into this notebook using the code below."
      ],
      "metadata": {
        "id": "YY0akMwPi0z-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "n2rjD0DKWoy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "9zDg2Xi0jQF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a matlab file, so we need to read it into python and extract the relevant variables. The code below will do this:"
      ],
      "metadata": {
        "id": "ptuDM5BljQty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mat = scipy.io.loadmat('neuralData_ReachingTask.mat')\n",
        "\n",
        "MO = int(mat['MO']) # movement onset\n",
        "GC = int(mat['GC']) # go cue\n",
        "Xneur = mat['X'].astype('float64') # C x T x N data matrix for C = 4 reach targets\n"
      ],
      "metadata": {
        "id": "Sqh3NbjWjXKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll start by selecting data recorded during movements to a single reach target."
      ],
      "metadata": {
        "id": "DLS5GwsdjfpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select a single target direction to fit\n",
        "ctarg = 0\n",
        "tstart = MO\n",
        "ytarg = (Xneur[ctarg,GC:,:].T - np.mean(Xneur[ctarg,tstart:,:].T,axis=1)[:, None])/np.std(Xneur[ctarg,tstart:,:])\n",
        "# selects data from tstart onwards"
      ],
      "metadata": {
        "id": "hbDBHAFQjZDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have loaded the data, we can get a sense of it's dimensionality and the kind of acitivity patterns motor cortex expresses during reaching by applying our old friend PCA to the data.\n",
        "\n",
        "1. Compute the data covariance matrix and plot its singular values.\n",
        "2. Plot the projections of the neural data along the leading principal components."
      ],
      "metadata": {
        "id": "X0ipT1sSjuqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PCs = np.linalg.svd(ytarg.dot(ytarg.T))[0]\n",
        "svds = np.linalg.svd(ytarg.dot(ytarg.T))[1]\n",
        "\n",
        "# look at dimensionality\n",
        "plt.plot(svds[:20]/sum(svds),'o-')\n",
        "plt.plot(np.cumsum(svds[:20]/sum(svds)),'o-')\n",
        "plt.plot(0.9*np.ones(20,),'k')\n",
        "\n",
        "plt.xlabel('dimension')\n",
        "plt.ylabel('fraction of variance explained')\n",
        "plt.show()\n",
        "\n",
        "# plot PC projections\n",
        "nPCs=4 # number of PCs to plot\n",
        "for i in range(nPCs):\n",
        "    plt.plot(PCs[:,i].dot(ytarg))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B-9A8HU_jsDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also have our EM implementation now, so we can do more than using PCA!\n",
        "\n",
        "Let's try to fit a dynamical system to our motor cortex data.\n",
        "1. Choose a latent dimensionality K. What is a good value for this?\n",
        "2. Initialize model paramters with random values\n",
        "3. Run the EM algorithm."
      ],
      "metadata": {
        "id": "TkXhOjoJj_z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 10\n",
        "d = ytarg.shape[0]\n",
        "\n",
        "x0 = np.random.randn(k)\n",
        "Q0 = np.eye(k)\n",
        "\n",
        "# initialize parameters\n",
        "Cinit = PCs[:,:k] # initialize with PCA\n",
        "Qinit = 0.1*np.eye(k)\n",
        "Rinit = 0.5*np.eye(d)\n",
        "Ainit = 0.9*np.eye(k)"
      ],
      "metadata": {
        "id": "uPlaEoBlj6lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run EM\n",
        "Ahat, Qhat, Chat, Rhat, x0hat, Q0hat, X, V, Vj, loglike = run_EM(ytarg, x0, Q0, Ainit, Qinit, Cinit, Rinit, maxiter=50)\n"
      ],
      "metadata": {
        "id": "1K-Li9aLkJdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Visualize the inferred posterior means. Do they look similar to the PC projections from before?\n"
      ],
      "metadata": {
        "id": "ck8MpAAXkwMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(X.T)\n",
        "plt.title('inferred posterior means $\\pm$ 2 std. dev')\n",
        "# plot the inferred posterior means\n",
        "\n",
        "# plot +/- 2 posterior std. around the means\n",
        "for i in range(k):\n",
        "    plt.fill_between(range(ytarg.shape[1]), X[i,:] + 2*np.sqrt(V[:,i,i]),\n",
        "                X[i,:]-2*np.sqrt(V[:,i,i]), alpha=0.2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qSAAsdo9ktko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Inspect the eigenvalues of the learned dynamics matrix. What do you notice about them? Is this what you expected based on our earlier exploration of properties of linear dynamics?"
      ],
      "metadata": {
        "id": "SNkinEY5k_8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evals = np.linalg.eig(Ahat)[0]\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(np.real(evals),np.imag(evals),'.')"
      ],
      "metadata": {
        "id": "Lxt9ipHSlA-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By now, we have a pretty good understanding of linear dynamics and we managed to implement a whole machine learning algorithm and applied it to neural data. But we also only looked at a single target direction.\n",
        "\n",
        "1. Look at the neural data for all targets using PCA. Do you think a single linear system could model this? Why? Why not?\n",
        "2. Try to run the fitted model forward from an initial condition for a single or multiple targets. Do the model predictions look good?"
      ],
      "metadata": {
        "id": "rYJ7ML7SlF3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PCs = np.linalg.svd(Xneur.reshape(-1,d).T.dot(Xneur.reshape(-1,d)))[0]\n",
        "svds = np.linalg.svd(Xneur.reshape(-1,d).T.dot(Xneur.reshape(-1,d)))[1]\n",
        "\n",
        "# look at dimensionality\n",
        "plt.plot(svds[:20]/sum(svds),'o-')\n",
        "plt.plot(np.cumsum(svds[:20]/sum(svds)),'o-')\n",
        "plt.plot(0.9*np.ones(20,),'k')\n",
        "\n",
        "plt.xlabel('dimension')\n",
        "plt.ylabel('fraction of variance explained')\n",
        "plt.show()\n",
        "\n",
        "# plot PC projections\n",
        "nPCs=4 # number of PCs to plot\n",
        "nTargets=4 # number of target directions\n",
        "for i in range(nPCs):\n",
        "  for c in range(nTargets):\n",
        "      plt.plot(PCs[:,i].dot(Xneur[c].T))\n",
        "  plt.title('PC %i'%i)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "6Faxwy1HlMXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xpred = predict_path(Ahat, x0hat, ytarg.shape[1])\n",
        "# function to predict forward from initial condition (you already wrote this above)\n",
        "\n",
        "plt.plot(X.T)\n",
        "for i in range(k):\n",
        "    plt.fill_between(range(ytarg.shape[1]), X[i,:] + 2*np.sqrt(V[:,i,i]),\n",
        "                X[i,:]-2*np.sqrt(V[:,i,i]), alpha=0.2)\n",
        "\n",
        "plt.plot(xpred.T,'--')\n",
        "plt.title('inferred posterior means and predictions forward from initial condition')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f2uSWXwYlVFY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BONUS EXERCISE: Adding an offset to our model**\n",
        "\n",
        "A limitation of linear systems is that they only have a single fixed point. This can be pretty restrictive, especially when movements to different targets produce pretty different-looking neural activity that might move to different points in the high-dimensional neural space.\n",
        "There also different epochs of the task (e.g. motor planning and motor execution) and it would be surprising if a single dynamical system without inputs could model this well. How would the system know when to transition from rest to movement?\n",
        "\n",
        "A simple but powerful extension to this simple linear model is to add an offset parameter that is condition specific and potentially time-varying:\n",
        "\n",
        "$$ x^c_{t+1} = A x^c_{t} + b^c_t + \\epsilon_t $$\n",
        "\n",
        "1. What does this kind of extension change about our model? What flexibility do we now have that we didn't have before? Do you think this will alleviate some of the problems we saw in our predictions forward from an initial condition?\n",
        "\n",
        "Let's try to get EM running for this model. We'll need to modify the E-Step to incorporate the extra paramter $b^c_t$, and derive a new update for $b^c$ for the M-step.\n",
        "\n",
        "It turns out that a piecewise constant $b^c_t$ is a good choice for our motor cortex data. This means that both the preparatory period (the monkey sees a target and plans his movement to it but doesn't move yet) and the movement period (the monkey executes his arm reach) can be described by thge same linear dynamics, but the fixed point of the linear system moves around across task epochs (preparator vs. movement) and conditions (target direction).\n"
      ],
      "metadata": {
        "id": "Ai4UxP7ile0B"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0xcfIRt0m0HA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}